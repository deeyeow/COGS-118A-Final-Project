{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers will be stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (algo, dataset, trial, metric)\n",
    "training_algo_data_all_metrics = np.zeros((3,4,5,3))\n",
    "\n",
    "# (depth, row, column)\n",
    "# (algo, dataset, mean metric)\n",
    "training_algo_data_mean_metrics = np.zeros((3,4,3))\n",
    "\n",
    "# (algo, dataset, trial, metric)\n",
    "algo_data_all_metrics = np.zeros((3,4,5,3))\n",
    "\n",
    "# (depth, row, column)\n",
    "# (algo, dataset, mean metric)\n",
    "algo_data_mean_metrics = np.zeros((3,4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset\n",
    "\n",
    "age: continuous. \n",
    "workclass: 8 (remove 1 after filtering)\n",
    "fnlwgt: continuous. \n",
    "education: 16 \n",
    "education-num: continuous. \n",
    "marital-status: 7\n",
    "occupation: 14 \n",
    "relationship: 6 \n",
    "race: 5 \n",
    "sex: 2 \n",
    "capital-gain: continuous. \n",
    "capital-loss: continuous. \n",
    "hours-per-week: continuous. \n",
    "native-country: 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt   education  education-num  \\\n",
       "0       39         State-gov   77516   Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311   Bachelors             13   \n",
       "2       38           Private  215646     HS-grad              9   \n",
       "3       53           Private  234721        11th              7   \n",
       "4       28           Private  338409   Bachelors             13   \n",
       "...    ...               ...     ...         ...            ...   \n",
       "32556   27           Private  257302  Assoc-acdm             12   \n",
       "32557   40           Private  154374     HS-grad              9   \n",
       "32558   58           Private  151910     HS-grad              9   \n",
       "32559   22           Private  201490     HS-grad              9   \n",
       "32560   52      Self-emp-inc  287927     HS-grad              9   \n",
       "\n",
       "           marital-status         occupation   relationship   race     sex  \\\n",
       "0           Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1      Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2                Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3      Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4      Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "32556  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "32557  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "32558             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "32559       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "32560  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0              2174             0              40  United-States  <=50K  \n",
       "1                 0             0              13  United-States  <=50K  \n",
       "2                 0             0              40  United-States  <=50K  \n",
       "3                 0             0              40  United-States  <=50K  \n",
       "4                 0             0              40           Cuba  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "32556             0             0              38  United-States  <=50K  \n",
       "32557             0             0              40  United-States   >50K  \n",
       "32558             0             0              40  United-States  <=50K  \n",
       "32559             0             0              20  United-States  <=50K  \n",
       "32560         15024             0              40  United-States   >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "        'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "adult = pd.read_csv('adult.data', names=col, skipinitialspace=True)\n",
    "adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "adult['income'].replace({'<=50K': 0, '>50K': 1}, inplace=True)\n",
    "adult = adult[adult['workclass'] != '?']\n",
    "adult = adult[adult['occupation'] != '?']\n",
    "adult = adult[adult['native-country'] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22654.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,  7508.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 388
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = adult\n",
    "plt.hist(df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22654\n",
       "1     7508\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and One-Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and download example data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define which columns should be encoded vs scaled\n",
    "columns_to_encode = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                    'relationship', 'race', 'sex', 'native-country']\n",
    "columns_to_scale  = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Instantiate encoder/scaler\n",
    "scaler = StandardScaler()\n",
    "ohe    = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Scale and Encode Separate Columns\n",
    "scaled_columns = scaler.fit_transform(df[columns_to_scale]) \n",
    "encoded_columns = ohe.fit_transform(df[columns_to_encode])\n",
    "\n",
    "y = df['income'].to_numpy()\n",
    "\n",
    "# Concatenate (Column-Bind) Processed Columns Back Together\n",
    "X = np.concatenate([scaled_columns, encoded_columns], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 27s, sys: 4.31 s, total: 6min 32s\n",
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "trials = 5\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    scores_train = np.zeros(len(scoring))\n",
    "    scores_test = np.zeros(len(scoring))\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=5000, \n",
    "                                                         stratify=y, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # LogReg\n",
    "    pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['saga'],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','sag'],\n",
    "                     'classifier__penalty': ['l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['liblinear'],\n",
    "                     'classifier__penalty': ['l1'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','saga','sag'],\n",
    "                     'classifier__penalty': ['none']}\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        penalty = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__penalty']\n",
    "        if 'classifier__C' in best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]:\n",
    "            C = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__C']\n",
    "        else:\n",
    "            C = 0\n",
    "        solver = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__solver']\n",
    "        clf = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=5000)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "\n",
    "        \n",
    "    training_algo_data_all_metrics[0][0][trial] = scores_train\n",
    "    algo_data_all_metrics[0][0][trial] = scores_test\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()],\n",
    "                     'classifier__weights': ['uniform','distance'],\n",
    "                     'classifier__n_neighbors': np.arange(1,105,4)},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        weights = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__weights']\n",
    "        n_neighbors = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_neighbors']\n",
    "        clf = KNeighborsClassifier(weights=weights, n_neighbors=n_neighbors)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[1][0][trial] = scores_train\n",
    "    algo_data_all_metrics[1][0][trial] = scores_test\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__max_features': [1,2,4,6,8,12,16,20],\n",
    "                     'classifier__n_estimators': [1024]},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        max_features = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__max_features']\n",
    "        n_estimators = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_estimators']\n",
    "        clf = RandomForestClassifier(max_features=max_features, n_estimators=n_estimators)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[2][0][trial] = scores_train\n",
    "    algo_data_all_metrics[2][0][trial] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.8484    , 0.66008969, 0.76242654],\n",
       "         [0.8532    , 0.6791958 , 0.76711266],\n",
       "         [0.847     , 0.66284707, 0.76068482],\n",
       "         [0.8488    , 0.6642984 , 0.75974845],\n",
       "         [0.8438    , 0.65549184, 0.76124311]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 0.74879117],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [0.8322    , 0.62694531, 1.        ],\n",
       "         [0.8322    , 0.61811561, 0.7431751 ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "         [0.84667356, 0.66446338, 0.76043991],\n",
       "         [0.84631587, 0.66388527, 0.76083769],\n",
       "         [0.84504411, 0.66454444, 0.76379805],\n",
       "         [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.83383674, 0.63684531, 0.74211571],\n",
       "         [0.83228678, 0.64127848, 0.754963  ],\n",
       "         [0.83232652, 0.6325233 , 0.74957605],\n",
       "         [0.83248549, 0.62549978, 0.74596865],\n",
       "         [0.83260472, 0.61882353, 0.74321715]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.8477466 , 0.6770244 , 0.77798718],\n",
       "         [0.84782609, 0.68064489, 0.78273313],\n",
       "         [0.8467133 , 0.67398519, 0.77411986],\n",
       "         [0.85199905, 0.68508006, 0.781964  ],\n",
       "         [0.85104523, 0.67314426, 0.77246699]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "0       2596   51   3  258    0   510  221  232  148  6279  ...   0   0   0   \n",
       "1       2590   56   2  212   -6   390  220  235  151  6225  ...   0   0   0   \n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "4       2595   45   2  153   -1   391  220  234  150  6172  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "581007  2396  153  20   85   17   108  240  237  118   837  ...   0   0   0   \n",
       "581008  2391  152  19   67   12    95  240  237  119   845  ...   0   0   0   \n",
       "581009  2386  159  17   60    7    90  236  241  130   854  ...   0   0   0   \n",
       "581010  2384  170  15   60    5    90  230  245  143   864  ...   0   0   0   \n",
       "581011  2383  165  13   60    4    67  231  244  141   875  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "0        0   0   0   0   0   0   5  \n",
       "1        0   0   0   0   0   0   5  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "4        0   0   0   0   0   0   5  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "581007   0   0   0   0   0   0   3  \n",
       "581008   0   0   0   0   0   0   3  \n",
       "581009   0   0   0   0   0   0   3  \n",
       "581010   0   0   0   0   0   0   3  \n",
       "581011   0   0   0   0   0   0   3  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "col = [\n",
    "    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "    'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n",
    "    'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area', 'Soil_Type', 'Cover_Type' \n",
    "]\n",
    "'''\n",
    "covtype = pd.read_csv('covtype.data', header=None)#.sample(n=10000)\n",
    "covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "covtype[54].replace({1:0, 3:0, 4:0, 5:0, 6:0, 7:0}, inplace=True)\n",
    "covtype[54].replace({2:1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([297711.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0., 283301.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 352
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = covtype\n",
    "plt.hist(df[54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    297711\n",
       "1    283301\n",
       "Name: 54, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[54].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and One-Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and download example data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define which columns should be encoded vs scaled\n",
    "columns_to_encode = range(10, 54)\n",
    "columns_to_scale  = range(0, 10)\n",
    "\n",
    "# Instantiate encoder/scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale and Encode Separate Columns\n",
    "scaled_columns = scaler.fit_transform(df[columns_to_scale]) \n",
    "encoded_columns = df[columns_to_encode]\n",
    "\n",
    "y = df[54].to_numpy()\n",
    "\n",
    "# Concatenate (Column-Bind) Processed Columns Back Together\n",
    "X = np.concatenate([scaled_columns, encoded_columns], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 25s, sys: 8.37 s, total: 38min 33s\n",
      "Wall time: 45min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "trials = 5\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    scores_train = np.zeros(len(scoring))\n",
    "    scores_test = np.zeros(len(scoring))\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=5000, \n",
    "                                                         stratify=y, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # LogReg\n",
    "    pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['saga'],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','sag'],\n",
    "                     'classifier__penalty': ['l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['liblinear'],\n",
    "                     'classifier__penalty': ['l1'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','saga','sag'],\n",
    "                     'classifier__penalty': ['none']}\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        penalty = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__penalty']\n",
    "        if 'classifier__C' in best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]:\n",
    "            C = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__C']\n",
    "        else:\n",
    "            C = 0\n",
    "        solver = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__solver']\n",
    "        clf = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=5000)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "\n",
    "    \n",
    "    training_algo_data_all_metrics[0][1][trial] = scores_train\n",
    "    algo_data_all_metrics[0][1][trial] = scores_test\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()],\n",
    "                     'classifier__weights': ['uniform','distance'],\n",
    "                     'classifier__n_neighbors': np.arange(1,105,4)},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        weights = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__weights']\n",
    "        n_neighbors = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_neighbors']\n",
    "        clf = KNeighborsClassifier(weights=weights, n_neighbors=n_neighbors)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[1][1][trial] = scores_train\n",
    "    algo_data_all_metrics[1][1][trial] = scores_test\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__max_features': [1,2,4,6,8,12,16,20],\n",
    "                     'classifier__n_estimators': [1024]},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        max_features = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__max_features']\n",
    "        n_estimators = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_estimators']\n",
    "        clf = RandomForestClassifier(max_features=max_features, n_estimators=n_estimators)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[2][1][trial] = scores_train\n",
    "    algo_data_all_metrics[2][1][trial] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.8484    , 0.66008969, 0.76242654],\n",
       "         [0.8532    , 0.6791958 , 0.76711266],\n",
       "         [0.847     , 0.66284707, 0.76068482],\n",
       "         [0.8488    , 0.6642984 , 0.75974845],\n",
       "         [0.8438    , 0.65549184, 0.76124311]],\n",
       "\n",
       "        [[0.7518    , 0.750653  , 0.75135796],\n",
       "         [0.7466    , 0.74665067, 0.75043179],\n",
       "         [0.744     , 0.74078574, 0.74434532],\n",
       "         [0.7648    , 0.76328502, 0.7644966 ],\n",
       "         [0.7576    , 0.75310401, 0.75802974]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 0.74879117],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [0.8322    , 0.62694531, 1.        ],\n",
       "         [0.8322    , 0.61811561, 0.7431751 ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "         [0.84667356, 0.66446338, 0.76043991],\n",
       "         [0.84631587, 0.66388527, 0.76083769],\n",
       "         [0.84504411, 0.66454444, 0.76379805],\n",
       "         [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "        [[0.75228815, 0.74993516, 0.75222159],\n",
       "         [0.74918578, 0.7486473 , 0.75048111],\n",
       "         [0.75040798, 0.74648295, 0.750498  ],\n",
       "         [0.75216315, 0.75016932, 0.75246744],\n",
       "         [0.75465442, 0.7541477 , 0.75511111]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.83383674, 0.63684531, 0.74211571],\n",
       "         [0.83228678, 0.64127848, 0.754963  ],\n",
       "         [0.83232652, 0.6325233 , 0.74957605],\n",
       "         [0.83248549, 0.62549978, 0.74596865],\n",
       "         [0.83260472, 0.61882353, 0.74321715]],\n",
       "\n",
       "        [[0.7836677 , 0.78201773, 0.78525701],\n",
       "         [0.7816851 , 0.78342892, 0.78236605],\n",
       "         [0.78294029, 0.78374858, 0.78351494],\n",
       "         [0.78414859, 0.78542447, 0.7847788 ],\n",
       "         [0.78024243, 0.78114556, 0.78082507]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.8477466 , 0.6770244 , 0.77798718],\n",
       "         [0.84782609, 0.68064489, 0.78273313],\n",
       "         [0.8467133 , 0.67398519, 0.77411986],\n",
       "         [0.85199905, 0.68508006, 0.781964  ],\n",
       "         [0.85104523, 0.67314426, 0.77246699]],\n",
       "\n",
       "        [[0.81863051, 0.81537426, 0.82259028],\n",
       "         [0.82628487, 0.82526865, 0.82766518],\n",
       "         [0.81642917, 0.81517183, 0.82485111],\n",
       "         [0.82426234, 0.82598462, 0.82436821],\n",
       "         [0.82950702, 0.82899417, 0.82987419]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>T</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0      T   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1      I   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2      D   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3      N   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4      G   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5  10\n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "19995  D   2   2   3   3   2   7   7   7   6   6   6   4   2   8   3   7\n",
       "19996  C   7  10   8   8   4   4   8   6   9  12   9  13   2   9   3   7\n",
       "19997  T   6   9   6   7   5   6  11   3   7  11   9   5   2  12   2   4\n",
       "19998  S   2   3   4   2   1   8   7   2   6  10   6   8   1   9   5   8\n",
       "19999  A   4   9   6   6   2   9   5   3   1   8   1   8   2   7   2   8\n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter = pd.read_csv('letter-recognition.data', header=None)\n",
    "letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10060\n",
       "1     9940\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# cleaning\n",
    "pos = [1]*13\n",
    "neg = [0]*13\n",
    "label = pos+neg\n",
    "asci = list(string.ascii_uppercase)\n",
    "\n",
    "to_replace = dict(zip(asci, label))\n",
    "\n",
    "letter[0].replace(to_replace, inplace=True)\n",
    "letter[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10060.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,  9940.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 352
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = letter\n",
    "plt.hist(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10060\n",
       "1     9940\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and download example data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define which columns should be encoded vs scaled\n",
    "columns_to_scale  = range(1, 17)\n",
    "\n",
    "# Instantiate encoder/scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale and Encode Separate Columns\n",
    "scaled_columns = scaler.fit_transform(df[columns_to_scale]) \n",
    "\n",
    "y = df[0].to_numpy()\n",
    "\n",
    "# Concatenate (Column-Bind) Processed Columns Back Together\n",
    "X = scaled_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 2.32 s, total: 1min 47s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "trials = 5\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    scores_train = np.zeros(len(scoring))\n",
    "    scores_test = np.zeros(len(scoring))\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=5000, \n",
    "                                                         stratify=y, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # LogReg\n",
    "    pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['saga'],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','sag'],\n",
    "                     'classifier__penalty': ['l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['liblinear'],\n",
    "                     'classifier__penalty': ['l1'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','saga','sag'],\n",
    "                     'classifier__penalty': ['none']}\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        penalty = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__penalty']\n",
    "        if 'classifier__C' in best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]:\n",
    "            C = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__C']\n",
    "        else:\n",
    "            C = 0\n",
    "        solver = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__solver']\n",
    "        clf = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=5000)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "\n",
    "        \n",
    "    training_algo_data_all_metrics[0][2][trial] = scores_train\n",
    "    algo_data_all_metrics[0][2][trial] = scores_test\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()],\n",
    "                     'classifier__weights': ['uniform','distance'],\n",
    "                     'classifier__n_neighbors': np.arange(1,105,4)},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        weights = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__weights']\n",
    "        n_neighbors = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_neighbors']\n",
    "        clf = KNeighborsClassifier(weights=weights, n_neighbors=n_neighbors)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[1][2][trial] = scores_train\n",
    "    algo_data_all_metrics[1][2][trial] = scores_test\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__max_features': [1,2,4,6,8,12,16,20],\n",
    "                     'classifier__n_estimators': [1024]},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        max_features = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__max_features']\n",
    "        n_estimators = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_estimators']\n",
    "        clf = RandomForestClassifier(max_features=max_features, n_estimators=n_estimators)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[2][2][trial] = scores_train\n",
    "    algo_data_all_metrics[2][2][trial] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.8484    , 0.66008969, 0.76242654],\n",
       "         [0.8532    , 0.6791958 , 0.76711266],\n",
       "         [0.847     , 0.66284707, 0.76068482],\n",
       "         [0.8488    , 0.6642984 , 0.75974845],\n",
       "         [0.8438    , 0.65549184, 0.76124311]],\n",
       "\n",
       "        [[0.7518    , 0.750653  , 0.75135796],\n",
       "         [0.7466    , 0.74665067, 0.75043179],\n",
       "         [0.744     , 0.74078574, 0.74434532],\n",
       "         [0.7648    , 0.76328502, 0.7644966 ],\n",
       "         [0.7576    , 0.75310401, 0.75802974]],\n",
       "\n",
       "        [[0.734     , 0.7369462 , 0.73409123],\n",
       "         [0.733     , 0.73347974, 0.73405763],\n",
       "         [0.7136    , 0.71474104, 0.71364969],\n",
       "         [0.7282    , 0.73051755, 0.72827782],\n",
       "         [0.731     , 0.734412  , 0.73091751]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 0.74879117],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [0.8322    , 0.62694531, 1.        ],\n",
       "         [0.8322    , 0.61811561, 0.7431751 ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "         [0.84667356, 0.66446338, 0.76043991],\n",
       "         [0.84631587, 0.66388527, 0.76083769],\n",
       "         [0.84504411, 0.66454444, 0.76379805],\n",
       "         [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "        [[0.75228815, 0.74993516, 0.75222159],\n",
       "         [0.74918578, 0.7486473 , 0.75048111],\n",
       "         [0.75040798, 0.74648295, 0.750498  ],\n",
       "         [0.75216315, 0.75016932, 0.75246744],\n",
       "         [0.75465442, 0.7541477 , 0.75511111]],\n",
       "\n",
       "        [[0.72266667, 0.72337878, 0.72304003],\n",
       "         [0.72993333, 0.73308295, 0.73023242],\n",
       "         [0.7242    , 0.72517106, 0.72451315],\n",
       "         [0.72306667, 0.7219917 , 0.72313657],\n",
       "         [0.7276    , 0.73228914, 0.72832502]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.83383674, 0.63684531, 0.74211571],\n",
       "         [0.83228678, 0.64127848, 0.754963  ],\n",
       "         [0.83232652, 0.6325233 , 0.74957605],\n",
       "         [0.83248549, 0.62549978, 0.74596865],\n",
       "         [0.83260472, 0.61882353, 0.74321715]],\n",
       "\n",
       "        [[0.7836677 , 0.78201773, 0.78525701],\n",
       "         [0.7816851 , 0.78342892, 0.78236605],\n",
       "         [0.78294029, 0.78374858, 0.78351494],\n",
       "         [0.78414859, 0.78542447, 0.7847788 ],\n",
       "         [0.78024243, 0.78114556, 0.78082507]],\n",
       "\n",
       "        [[0.9558    , 0.95545253, 0.94773545],\n",
       "         [0.95326667, 0.95305699, 0.94344796],\n",
       "         [0.95126667, 0.9507711 , 0.94639967],\n",
       "         [0.95373333, 0.9535413 , 0.94856748],\n",
       "         [0.95286667, 0.95244501, 0.94687635]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.8477466 , 0.6770244 , 0.77798718],\n",
       "         [0.84782609, 0.68064489, 0.78273313],\n",
       "         [0.8467133 , 0.67398519, 0.77411986],\n",
       "         [0.85199905, 0.68508006, 0.781964  ],\n",
       "         [0.85104523, 0.67314426, 0.77246699]],\n",
       "\n",
       "        [[0.81863051, 0.81537426, 0.82259028],\n",
       "         [0.82628487, 0.82526865, 0.82766518],\n",
       "         [0.81642917, 0.81517183, 0.82485111],\n",
       "         [0.82426234, 0.82598462, 0.82436821],\n",
       "         [0.82950702, 0.82899417, 0.82987419]],\n",
       "\n",
       "        [[0.94893333, 0.94784489, 0.94929151],\n",
       "         [0.94486667, 0.94423295, 0.94435893],\n",
       "         [0.94766667, 0.9480188 , 0.94746158],\n",
       "         [0.94713333, 0.94684697, 0.94648354],\n",
       "         [0.94413333, 0.94413183, 0.94480041]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dota Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Team won the game (1 or -1)\n",
    "2. Cluster ID (related to location)\n",
    "3. Game mode (eg All Pick)\n",
    "4. Game type (eg. Ranked)\n",
    "5 - end: Each element is an indicator for a hero. Value of 1 indicates that a player from team '1' played as that hero and '-1' for the other team. Hero can be selected by only one player each game. This means that each row has five '1' and five '-1' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92645</th>\n",
       "      <td>-1</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92646</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92647</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92648</th>\n",
       "      <td>-1</td>\n",
       "      <td>185</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92649</th>\n",
       "      <td>-1</td>\n",
       "      <td>204</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92650 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  107  108  109  \\\n",
       "0       -1  223    2    2    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        1  152    2    2    0    0    0    1    0   -1  ...    0    0    0   \n",
       "2        1  131    2    2    0    0    0    1    0   -1  ...    0    0    0   \n",
       "3        1  154    2    2    0    0    0    0    0    0  ...   -1    0    0   \n",
       "4       -1  171    2    3    0    0    0    0    0   -1  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "92645   -1  154    2    3    1    0    0   -1    0    0  ...    0    0    0   \n",
       "92646    1  154    2    2    0    0    0    0   -1    0  ...    1    0    0   \n",
       "92647    1  111    2    3    0    0    0    0    0    0  ...    0    0    0   \n",
       "92648   -1  185    2    2    0    0    0    0    0    1  ...    0    0    0   \n",
       "92649   -1  204    2    2    0   -1    0    0    1    0  ...    0    0    0   \n",
       "\n",
       "       110  111  112  113  114  115  116  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "92645    0    0    0    0    0    0    0  \n",
       "92646    0    0    0    0    0    0    0  \n",
       "92647    0    0    0    0    0    0    0  \n",
       "92648    0    0    0    0    0    0    0  \n",
       "92649    0    0    0    0    0    0    0  \n",
       "\n",
       "[92650 rows x 117 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dota = pd.read_csv('dota2Train.csv', header=None)\n",
    "dota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "dota[0].replace({-1:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([43868.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 48782.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 352
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = dota\n",
    "plt.hist(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    48782\n",
       "0    43868\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and One-Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and download example data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define which columns should be encoded vs scaled\n",
    "columns_to_encode = range(2, 4)\n",
    "columns_to_scale  = 1\n",
    "\n",
    "# Instantiate encoder/scaler\n",
    "scaler = StandardScaler()\n",
    "ohe    = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Scale and Encode Separate Columns\n",
    "scaled_columns = scaler.fit_transform(df[columns_to_scale][:,np.newaxis]) \n",
    "encoded_columns = ohe.fit_transform(df[columns_to_encode])\n",
    "\n",
    "y = df[0].to_numpy()\n",
    "\n",
    "# Concatenate (Column-Bind) Processed Columns Back Together\n",
    "X = np.concatenate([scaled_columns, encoded_columns], axis=1)\n",
    "X = np.concatenate([X, df[range(4, 117)]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 41s, sys: 6.54 s, total: 22min 48s\n",
      "Wall time: 36min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "trials = 5\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "\n",
    "for trial in range(trials):\n",
    "    \n",
    "    scores_train = np.zeros(len(scoring))\n",
    "    scores_test = np.zeros(len(scoring))\n",
    "\n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=5000, \n",
    "                                                         stratify=y, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # LogReg\n",
    "    pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['saga'],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','sag'],\n",
    "                     'classifier__penalty': ['l2'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['liblinear'],\n",
    "                     'classifier__penalty': ['l1'],\n",
    "                     'classifier__C': np.logspace(-8, 4, 13)},\n",
    "                    {'classifier': [LogisticRegression(max_iter=5000)],\n",
    "                     'classifier__solver': ['newton-cg','lbfgs','saga','sag'],\n",
    "                     'classifier__penalty': ['none']}\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        penalty = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__penalty']\n",
    "        if 'classifier__C' in best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]:\n",
    "            C = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__C']\n",
    "        else:\n",
    "            C = 0\n",
    "        solver = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__solver']\n",
    "        clf = LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=5000)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "\n",
    "        \n",
    "    training_algo_data_all_metrics[0][3][trial] = scores_train\n",
    "    algo_data_all_metrics[0][3][trial] = scores_test\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    pipe = Pipeline([('classifier', KNeighborsClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [KNeighborsClassifier()],\n",
    "                     'classifier__weights': ['uniform','distance'],\n",
    "                     'classifier__n_neighbors': np.arange(1,105,4)},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        weights = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__weights']\n",
    "        n_neighbors = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_neighbors']\n",
    "        clf = KNeighborsClassifier(weights=weights, n_neighbors=n_neighbors)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[1][3][trial] = scores_train\n",
    "    algo_data_all_metrics[1][3][trial] = scores_test\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__max_features': [1,2,4,6,8,12,16,20],\n",
    "                     'classifier__n_estimators': [1024]},\n",
    "                    ]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                       scoring=scoring, refit=False,\n",
    "                       verbose=0, n_jobs=-1)\n",
    "\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "    for i,metric in enumerate(scoring):\n",
    "        max_features = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__max_features']\n",
    "        n_estimators = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_'+metric]) ]['classifier__n_estimators']\n",
    "        clf = RandomForestClassifier(max_features=max_features, n_estimators=n_estimators)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        if i == 0:\n",
    "            score_train = accuracy_score(y_train, y_pred_train)\n",
    "            score_test = accuracy_score(y_test, y_pred_test)\n",
    "        elif i == 1:\n",
    "            score_train = f1_score(y_train, y_pred_train)\n",
    "            score_test = f1_score(y_test, y_pred_test)\n",
    "        else: \n",
    "            score_train = roc_auc_score(y_train, y_pred_train)\n",
    "            score_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "        scores_train[i] = score_train\n",
    "        scores_test[i] = score_test\n",
    "        \n",
    "    training_algo_data_all_metrics[2][3][trial] = scores_train\n",
    "    algo_data_all_metrics[2][3][trial] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.8484    , 0.66008969, 0.76242654],\n",
       "         [0.8532    , 0.6791958 , 0.76711266],\n",
       "         [0.847     , 0.66284707, 0.76068482],\n",
       "         [0.8488    , 0.6642984 , 0.75974845],\n",
       "         [0.8438    , 0.65549184, 0.76124311]],\n",
       "\n",
       "        [[0.7518    , 0.750653  , 0.75135796],\n",
       "         [0.7466    , 0.74665067, 0.75043179],\n",
       "         [0.744     , 0.74078574, 0.74434532],\n",
       "         [0.7648    , 0.76328502, 0.7644966 ],\n",
       "         [0.7576    , 0.75310401, 0.75802974]],\n",
       "\n",
       "        [[0.734     , 0.7369462 , 0.73409123],\n",
       "         [0.733     , 0.73347974, 0.73405763],\n",
       "         [0.7136    , 0.71474104, 0.71364969],\n",
       "         [0.7282    , 0.73051755, 0.72827782],\n",
       "         [0.731     , 0.734412  , 0.73091751]],\n",
       "\n",
       "        [[0.631     , 0.66424022, 0.6275092 ],\n",
       "         [0.6048    , 0.64676439, 0.60017849],\n",
       "         [0.622     , 0.65723613, 0.61925392],\n",
       "         [0.6326    , 0.66484218, 0.62896653],\n",
       "         [0.6044    , 0.66130137, 0.60233355]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 0.74879117],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [0.8322    , 0.62694531, 1.        ],\n",
       "         [0.8322    , 0.61811561, 0.7431751 ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[0.5996    , 0.6622807 , 1.        ],\n",
       "         [0.581     , 0.63873082, 1.        ],\n",
       "         [0.5784    , 0.63604972, 1.        ],\n",
       "         [0.5806    , 0.64571718, 1.        ],\n",
       "         [0.5732    , 0.64692257, 1.        ]]],\n",
       "\n",
       "\n",
       "       [[[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]],\n",
       "\n",
       "        [[1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ],\n",
       "         [1.        , 1.        , 1.        ]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "         [0.84667356, 0.66446338, 0.76043991],\n",
       "         [0.84631587, 0.66388527, 0.76083769],\n",
       "         [0.84504411, 0.66454444, 0.76379805],\n",
       "         [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "        [[0.75228815, 0.74993516, 0.75222159],\n",
       "         [0.74918578, 0.7486473 , 0.75048111],\n",
       "         [0.75040798, 0.74648295, 0.750498  ],\n",
       "         [0.75216315, 0.75016932, 0.75246744],\n",
       "         [0.75465442, 0.7541477 , 0.75511111]],\n",
       "\n",
       "        [[0.72266667, 0.72337878, 0.72304003],\n",
       "         [0.72993333, 0.73308295, 0.73023242],\n",
       "         [0.7242    , 0.72517106, 0.72451315],\n",
       "         [0.72306667, 0.7219917 , 0.72313657],\n",
       "         [0.7276    , 0.73228914, 0.72832502]],\n",
       "\n",
       "        [[0.58687963, 0.62380918, 0.58331325],\n",
       "         [0.58471192, 0.62680446, 0.58035078],\n",
       "         [0.58713063, 0.62883637, 0.58301562],\n",
       "         [0.58399315, 0.62037085, 0.58049612],\n",
       "         [0.58351398, 0.64698373, 0.58214758]]],\n",
       "\n",
       "\n",
       "       [[[0.83383674, 0.63684531, 0.74211571],\n",
       "         [0.83228678, 0.64127848, 0.754963  ],\n",
       "         [0.83232652, 0.6325233 , 0.74957605],\n",
       "         [0.83248549, 0.62549978, 0.74596865],\n",
       "         [0.83260472, 0.61882353, 0.74321715]],\n",
       "\n",
       "        [[0.7836677 , 0.78201773, 0.78525701],\n",
       "         [0.7816851 , 0.78342892, 0.78236605],\n",
       "         [0.78294029, 0.78374858, 0.78351494],\n",
       "         [0.78414859, 0.78542447, 0.7847788 ],\n",
       "         [0.78024243, 0.78114556, 0.78082507]],\n",
       "\n",
       "        [[0.9558    , 0.95545253, 0.94773545],\n",
       "         [0.95326667, 0.95305699, 0.94344796],\n",
       "         [0.95126667, 0.9507711 , 0.94639967],\n",
       "         [0.95373333, 0.9535413 , 0.94856748],\n",
       "         [0.95286667, 0.95244501, 0.94687635]],\n",
       "\n",
       "        [[0.5527895 , 0.62281326, 0.54449757],\n",
       "         [0.54516828, 0.60737852, 0.53836272],\n",
       "         [0.54552196, 0.61348134, 0.53825769],\n",
       "         [0.54992584, 0.61889811, 0.54177283],\n",
       "         [0.54783799, 0.62953131, 0.53773578]]],\n",
       "\n",
       "\n",
       "       [[[0.8477466 , 0.6770244 , 0.77798718],\n",
       "         [0.84782609, 0.68064489, 0.78273313],\n",
       "         [0.8467133 , 0.67398519, 0.77411986],\n",
       "         [0.85199905, 0.68508006, 0.781964  ],\n",
       "         [0.85104523, 0.67314426, 0.77246699]],\n",
       "\n",
       "        [[0.81863051, 0.81537426, 0.82259028],\n",
       "         [0.82628487, 0.82526865, 0.82766518],\n",
       "         [0.81642917, 0.81517183, 0.82485111],\n",
       "         [0.82426234, 0.82598462, 0.82436821],\n",
       "         [0.82950702, 0.82899417, 0.82987419]],\n",
       "\n",
       "        [[0.94893333, 0.94784489, 0.94929151],\n",
       "         [0.94486667, 0.94423295, 0.94435893],\n",
       "         [0.94766667, 0.9480188 , 0.94746158],\n",
       "         [0.94713333, 0.94684697, 0.94648354],\n",
       "         [0.94413333, 0.94413183, 0.94480041]],\n",
       "\n",
       "        [[0.56957216, 0.62587125, 0.5629031 ],\n",
       "         [0.55989732, 0.61461205, 0.55554677],\n",
       "         [0.56716486, 0.63026833, 0.55769176],\n",
       "         [0.56579578, 0.62837844, 0.55926904],\n",
       "         [0.56499715, 0.61872144, 0.55884609]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each algo\n",
    "for i in range(3):\n",
    "    # for each dataset\n",
    "    for j in range(4):\n",
    "        training_algo_data_mean_metrics[i][j] = training_algo_data_all_metrics[i][j].mean(axis=0)\n",
    "        algo_data_mean_metrics[i][j] = algo_data_all_metrics[i][j].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.84824   , 0.66438456, 0.76224311],\n",
       "        [0.75296   , 0.75089569, 0.75373228],\n",
       "        [0.72796   , 0.7300193 , 0.72819878],\n",
       "        [0.61896   , 0.65887686, 0.61564834]],\n",
       "\n",
       "       [[0.93288   , 0.84901218, 0.89839325],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [0.58256   , 0.6459402 , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_data_mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.84633177, 0.66271911, 0.76250538],\n",
       "        [0.75173989, 0.74987648, 0.75215585],\n",
       "        [0.72549333, 0.72718273, 0.72584944],\n",
       "        [0.58524586, 0.62936092, 0.58186467]],\n",
       "\n",
       "       [[0.83270805, 0.63099408, 0.74716811],\n",
       "        [0.78253682, 0.78315305, 0.78334837],\n",
       "        [0.95338667, 0.95305339, 0.94660538],\n",
       "        [0.54824872, 0.61842051, 0.54012532]],\n",
       "\n",
       "       [[0.84906605, 0.67797576, 0.77785423],\n",
       "        [0.82302278, 0.82215871, 0.82586979],\n",
       "        [0.94654667, 0.94621509, 0.94647919],\n",
       "        [0.56548545, 0.6235703 , 0.55885135]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of metrics over all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (algo, metric)\n",
    "training_algo_mean_metrics = np.zeros((3,3))\n",
    "algo_mean_metrics = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    training_algo_mean_metrics[i] = training_algo_data_mean_metrics[i].mean(axis=0)\n",
    "    algo_mean_metrics[i] = algo_data_mean_metrics[i].mean(axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73703   , 0.7010441 , 0.71495563],\n",
       "       [0.87886   , 0.8737381 , 0.97459831],\n",
       "       [1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72720272, 0.69228481, 0.70559383],\n",
       "       [0.77922006, 0.74640526, 0.7543118 ],\n",
       "       [0.79603024, 0.76747996, 0.77726364]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "        [0.84667356, 0.66446338, 0.76043991],\n",
       "        [0.84631587, 0.66388527, 0.76083769],\n",
       "        [0.84504411, 0.66454444, 0.76379805],\n",
       "        [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "       [[0.75228815, 0.74993516, 0.75222159],\n",
       "        [0.74918578, 0.7486473 , 0.75048111],\n",
       "        [0.75040798, 0.74648295, 0.750498  ],\n",
       "        [0.75216315, 0.75016932, 0.75246744],\n",
       "        [0.75465442, 0.7541477 , 0.75511111]],\n",
       "\n",
       "       [[0.72266667, 0.72337878, 0.72304003],\n",
       "        [0.72993333, 0.73308295, 0.73023242],\n",
       "        [0.7242    , 0.72517106, 0.72451315],\n",
       "        [0.72306667, 0.7219917 , 0.72313657],\n",
       "        [0.7276    , 0.73228914, 0.72832502]],\n",
       "\n",
       "       [[0.58687963, 0.62380918, 0.58331325],\n",
       "        [0.58471192, 0.62680446, 0.58035078],\n",
       "        [0.58713063, 0.62883637, 0.58301562],\n",
       "        [0.58399315, 0.62037085, 0.58049612],\n",
       "        [0.58351398, 0.64698373, 0.58214758]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84619665, 0.84667356, 0.84631587, 0.84504411, 0.84742866,\n",
       "       0.75228815, 0.74918578, 0.75040798, 0.75216315, 0.75465442,\n",
       "       0.72266667, 0.72993333, 0.7242    , 0.72306667, 0.7276    ,\n",
       "       0.58687963, 0.58471192, 0.58713063, 0.58399315, 0.58351398])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first algo, all datasets, first metric\n",
    "algo_data_all_metrics[0][:,:,0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8477466 , 0.84782609, 0.8467133 , 0.85199905, 0.85104523,\n",
       "       0.81863051, 0.82628487, 0.81642917, 0.82426234, 0.82950702,\n",
       "       0.94893333, 0.94486667, 0.94766667, 0.94713333, 0.94413333,\n",
       "       0.56957216, 0.55989732, 0.56716486, 0.56579578, 0.56499715])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best\n",
    "# third algo, all datasets, first metric\n",
    "algo_data_all_metrics[2][:,:,0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-2.1706411296879, pvalue=0.04283710466495411)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_rel(algo_data_all_metrics[0][:,:,0].flatten(), algo_data_all_metrics[1][:,:,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_score_metrics = np.zeros((3,3))\n",
    "p_value_metrics = np.zeros((3,3))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        t_score_metrics[i][j] = ttest_rel(algo_data_all_metrics[i][:,:,j].flatten(), \n",
    "                                         algo_data_all_metrics[2][:,:,j].flatten())[0]\n",
    "        p_value_metrics[i][j] = ttest_rel(algo_data_all_metrics[i][:,:,j].flatten(), \n",
    "                                         algo_data_all_metrics[2][:,:,j].flatten())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.18695942, -3.71781761, -3.36965411],\n",
       "       [-4.28652637, -3.90048023, -6.21651152],\n",
       "       [        nan,         nan,         nan]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_score_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.85398599e-03, 1.45924706e-03, 3.21780315e-03],\n",
       "       [3.98423258e-04, 9.61754296e-04, 5.68759043e-06],\n",
       "       [           nan,            nan,            nan]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of datasets over all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (algo, dataset)\n",
    "training_algo_mean_datasets = np.zeros((3,4))\n",
    "algo_mean_datasets = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        training_algo_mean_datasets[i][j] = training_algo_data_mean_metrics[i][j].mean()\n",
    "        algo_mean_datasets[i][j] = algo_data_mean_metrics[i][j].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75828922, 0.75252932, 0.72872603, 0.63116173],\n",
       "       [0.89342848, 1.        , 1.        , 0.7428334 ],\n",
       "       [1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_algo_mean_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75718542, 0.75125741, 0.72617517, 0.59882382],\n",
       "       [0.73695675, 0.78301275, 0.95101515, 0.56893151],\n",
       "       [0.76829868, 0.82368376, 0.94641365, 0.5826357 ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_mean_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.84619665, 0.66022827, 0.7640097 ],\n",
       "        [0.84667356, 0.66446338, 0.76043991],\n",
       "        [0.84631587, 0.66388527, 0.76083769],\n",
       "        [0.84504411, 0.66454444, 0.76379805],\n",
       "        [0.84742866, 0.66047417, 0.76344157]],\n",
       "\n",
       "       [[0.75228815, 0.74993516, 0.75222159],\n",
       "        [0.74918578, 0.7486473 , 0.75048111],\n",
       "        [0.75040798, 0.74648295, 0.750498  ],\n",
       "        [0.75216315, 0.75016932, 0.75246744],\n",
       "        [0.75465442, 0.7541477 , 0.75511111]],\n",
       "\n",
       "       [[0.72266667, 0.72337878, 0.72304003],\n",
       "        [0.72993333, 0.73308295, 0.73023242],\n",
       "        [0.7242    , 0.72517106, 0.72451315],\n",
       "        [0.72306667, 0.7219917 , 0.72313657],\n",
       "        [0.7276    , 0.73228914, 0.72832502]],\n",
       "\n",
       "       [[0.58687963, 0.62380918, 0.58331325],\n",
       "        [0.58471192, 0.62680446, 0.58035078],\n",
       "        [0.58713063, 0.62883637, 0.58301562],\n",
       "        [0.58399315, 0.62037085, 0.58049612],\n",
       "        [0.58351398, 0.64698373, 0.58214758]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_data_all_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_score_datasets = np.zeros((3,4))\n",
    "p_value_datasets = np.zeros((3,4))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        t_score_datasets[i][j] = ttest_rel(algo_data_all_metrics[i][j].flatten(), \n",
    "                                           algo_data_all_metrics[2][j].flatten())[0]\n",
    "        p_value_datasets[i][j] = ttest_rel(algo_data_all_metrics[i][j].flatten(), \n",
    "                                           algo_data_all_metrics[2][j].flatten())[1]\n",
    "\n",
    "for i in range(3):\n",
    "    t_score_datasets[i][2] = ttest_rel(algo_data_all_metrics[i][2].flatten(),\n",
    "                                      algo_data_all_metrics[1][2].flatten())[0]\n",
    "    p_value_datasets[i][2] = ttest_rel(algo_data_all_metrics[i][2].flatten(),\n",
    "                                      algo_data_all_metrics[1][2].flatten())[1]\n",
    "    \n",
    "    \n",
    "for i in range(3):\n",
    "    t_score_datasets[i][3] = ttest_rel(algo_data_all_metrics[i][3].flatten(),\n",
    "                                      algo_data_all_metrics[0][3].flatten())[0]\n",
    "    p_value_datasets[i][3] = ttest_rel(algo_data_all_metrics[i][3].flatten(),\n",
    "                                      algo_data_all_metrics[0][3].flatten())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -5.99682381,  -67.76020431, -155.78257222,           nan],\n",
       "       [  -8.5478719 ,  -27.18470381,           nan,   -7.71455071],\n",
       "       [          nan,           nan,   -4.67825994,   -5.62118272]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_score_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.27260068e-05, 5.03153590e-19, 4.43726856e-24,            nan],\n",
       "       [6.28165742e-07, 1.62176638e-13,            nan, 2.08498964e-06],\n",
       "       [           nan,            nan, 3.55520944e-04, 6.30513945e-05]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
